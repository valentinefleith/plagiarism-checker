backend/model/bert_phrases_classifier/tokenizer_config.json filter=lfs diff=lfs merge=lfs -text
backend/model/bert_phrases_classifier/vocab.txt filter=lfs diff=lfs merge=lfs -text
backend/model/label_encoder/byteorder filter=lfs diff=lfs merge=lfs -text
backend/model/label_encoder/data.pkl filter=lfs diff=lfs merge=lfs -text
backend/model/label_encoder/version filter=lfs diff=lfs merge=lfs -text
backend/model/bert_phrases_classifier/config.json filter=lfs diff=lfs merge=lfs -text
backend/model/bert_phrases_classifier/model.safetensors filter=lfs diff=lfs merge=lfs -text
backend/model/bert_phrases_classifier/special_tokens_map.json filter=lfs diff=lfs merge=lfs -text
backend/model/bert_phrases_classifier filter=lfs diff=lfs merge=lfs -text
backend/model/label_encoder.pth filter=lfs diff=lfs merge=lfs -text
